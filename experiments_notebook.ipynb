{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time_lstm import T1TimeLSTM, T2TimeLSTM\n",
    "from dataset_delay_predictions import Dataset_Delay_Prediction, Dataset_Delay_Prediction_from_list, DatasetDelayPredictionStackOverflow\n",
    "from time_dependant_representation import TimeDepMasking, TimeDepJointEmbedding\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Masking, Layer, LSTM\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [\n",
    "    '2019-10_11_12-dataset_delay_prediction',\n",
    "    'stack-overflow-dataset'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading pickle file...\n"
     ]
    }
   ],
   "source": [
    "dataset_name = dataset_names[0]\n",
    "pickle_filename = dataset_name.replace('/', '-')\n",
    "if os.path.exists(pickle_filename):\n",
    "    print('Reading pickle file...')\n",
    "    dataset = pickle.load(open(pickle_filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5307, 1)\n"
     ]
    }
   ],
   "source": [
    "training_set_length = dataset.training_set_length\n",
    "X_train = dataset.full_features_dt[dataset.training_set_length:]\n",
    "seqlen = dataset.full_seqlen[dataset.training_set_length:]\n",
    "y_train = np.array(dataset.full_values[dataset.training_set_length:])\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4375, 1)\n"
     ]
    }
   ],
   "source": [
    "training_set_length = int(1/5 * len(dataset.full_features))\n",
    "X_train = dataset.full_features[training_set_length:]\n",
    "seqlen = dataset.full_seqlen[training_set_length:]\n",
    "y_train = np.array(dataset.full_values[training_set_length:])\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_event = dataset.number_of_events\n",
    "X_train_bis = []\n",
    "number_of_event = len(X_train[0][0])\n",
    "for idx in range(len(X_train)):\n",
    "    seq = X_train[idx]\n",
    "    ts_list = [[a[-1]] for a in seq]\n",
    "    X_train_bis.append(ts_list)\n",
    "\n",
    "padding_value = 0.123456789\n",
    "padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(X_train_bis,\n",
    "                                                              padding='post',\n",
    "                                                              value=padding_value,\n",
    "                                                              dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4375, 410, 1)\n"
     ]
    }
   ],
   "source": [
    "print(padded_inputs.shape)\n",
    "lstm_units = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = Sequential()\n",
    "regressor.add(Masking(mask_value=padding_value))\n",
    "regressor.add(LSTM(units=lstm_units))\n",
    "regressor.add(Dense(units=y_train.shape[1], activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4375 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x7f3b9e3fb730>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/charles/anaconda3/envs/tf-2.0/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\", line 541, in __del__\n",
      "    handle=self._handle, deleter=self._deleter)\n",
      "  File \"/home/charles/anaconda3/envs/tf-2.0/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py\", line 1157, in delete_iterator\n",
      "    \"DeleteIterator\", handle=handle, deleter=deleter, name=name)\n",
      "  File \"/home/charles/anaconda3/envs/tf-2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 793, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/charles/anaconda3/envs/tf-2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 544, in create_op\n",
      "    inp = self.capture(inp)\n",
      "  File \"/home/charles/anaconda3/envs/tf-2.0/lib/python3.7/site-packages/tensorflow_core/python/ops/while_v2.py\", line 919, in capture\n",
      "    (str(tensor), _graph_name(tensor.graph)))\n",
      "  File \"/home/charles/anaconda3/envs/tf-2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 699, in __str__\n",
      "    self.name,\n",
      "  File \"/home/charles/anaconda3/envs/tf-2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 416, in name\n",
      "    if not self._op.name:\n",
      "  File \"/home/charles/anaconda3/envs/tf-2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1882, in name\n",
      "    return c_api.TF_OperationName(self._c_op)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x7f3b9e3fb730>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/charles/anaconda3/envs/tf-2.0/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\", line 541, in __del__\n",
      "    handle=self._handle, deleter=self._deleter)\n",
      "  File \"/home/charles/anaconda3/envs/tf-2.0/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py\", line 1157, in delete_iterator\n",
      "    \"DeleteIterator\", handle=handle, deleter=deleter, name=name)\n",
      "  File \"/home/charles/anaconda3/envs/tf-2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 793, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/charles/anaconda3/envs/tf-2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 544, in create_op\n",
      "    inp = self.capture(inp)\n",
      "  File \"/home/charles/anaconda3/envs/tf-2.0/lib/python3.7/site-packages/tensorflow_core/python/ops/while_v2.py\", line 919, in capture\n",
      "    (str(tensor), _graph_name(tensor.graph)))\n",
      "  File \"/home/charles/anaconda3/envs/tf-2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 699, in __str__\n",
      "    self.name,\n",
      "  File \"/home/charles/anaconda3/envs/tf-2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 416, in name\n",
      "    if not self._op.name:\n",
      "  File \"/home/charles/anaconda3/envs/tf-2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1882, in name\n",
      "    return c_api.TF_OperationName(self._c_op)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4375/4375 - 33s - loss: 0.0268\n",
      "Epoch 2/5\n",
      "4375/4375 - 12s - loss: 0.0241\n",
      "Epoch 3/5\n",
      "4375/4375 - 12s - loss: 0.0243\n",
      "Epoch 4/5\n",
      "4375/4375 - 13s - loss: 0.0237\n",
      "Epoch 5/5\n",
      "4375/4375 - 12s - loss: 0.0234\n"
     ]
    }
   ],
   "source": [
    "\n",
    "regressor.compile(optimizer='adam', loss='mean_squared_error')\n",
    "history = regressor.fit(padded_inputs, y_train, batch_size=50, epochs=number_of_epochs, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = dataset.full_features[:training_set_length]\n",
    "seqlen_test = dataset.full_seqlen[:training_set_length]\n",
    "y_test = np.array(dataset.full_values[:training_set_length])\n",
    "X_test_bis = []\n",
    "number_of_event = len(X_train[0][0])\n",
    "for idx in range(training_set_length):\n",
    "    seq = X_test[idx]\n",
    "    ts_list = [[a[-1]] for a in seq]\n",
    "    X_test_bis.append(ts_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02284944]\n"
     ]
    }
   ],
   "source": [
    "padding_value = 0.123456789\n",
    "padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(X_test_bis,\n",
    "                                                              padding='post',\n",
    "                                                              value=padding_value,\n",
    "                                                              dtype='float32')\n",
    "pred = regressor.predict(padded_inputs)\n",
    "mean_sum_of_squares = sum([(y_test[i] - pred[i])**2 for i in range(training_set_length)])/training_set_length\n",
    "print(mean_sum_of_squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5307, 735, 23)\n",
      "Train on 5307 samples\n",
      "Epoch 1/5\n",
      "5307/5307 - 35s - loss: 0.2294\n",
      "Epoch 2/5\n",
      "5307/5307 - 28s - loss: 0.2290\n",
      "Epoch 3/5\n",
      "5307/5307 - 29s - loss: 0.2290\n",
      "Epoch 4/5\n",
      "5307/5307 - 30s - loss: 0.2297\n",
      "Epoch 5/5\n",
      "5307/5307 - 30s - loss: 0.2288\n"
     ]
    }
   ],
   "source": [
    "padding_value = 0.123456789\n",
    "padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(X_train,\n",
    "                                                              padding='post',\n",
    "                                                              value=padding_value,\n",
    "                                                              dtype='float32')\n",
    "\n",
    "print(padded_inputs.shape)\n",
    "regressor = Sequential()\n",
    "regressor.add(Masking(mask_value=padding_value))\n",
    "regressor.add(LSTM(units=lstm_units))\n",
    "regressor.add(Dense(units=y_train.shape[1], activation='sigmoid'))\n",
    "\n",
    "\n",
    "regressor.compile(optimizer='adam', loss='mean_squared_error')\n",
    "history = regressor.fit(padded_inputs, y_train, batch_size=50, epochs=number_of_epochs, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.22875992]\n"
     ]
    }
   ],
   "source": [
    "X_test = dataset.full_features_dt[:dataset.training_set_length]\n",
    "seqlen_test = dataset.full_seqlen[:dataset.training_set_length]\n",
    "y_test = np.array(dataset.full_values[:dataset.training_set_length])\n",
    "\n",
    "padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(X_test,\n",
    "                                                              padding='post',\n",
    "                                                              value=padding_value,\n",
    "                                                              dtype='float32')\n",
    "pred = regressor.predict(padded_inputs)\n",
    "mean_sum_of_squares = sum([(y_test[i] - pred[i])**2 for i in range(dataset.training_set_length)])/dataset.training_set_length\n",
    "print(mean_sum_of_squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1TimeLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5307, 735, 23)\n",
      "Train on 5307 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/charles/.local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-62-39502405b399>\", line 12, in <module>\n",
      "    history = regressor.fit(padded_inputs, y_train, batch_size=1, epochs=number_of_epochs, verbose=2)\n",
      "  File \"/home/charles/anaconda3/envs/tf-2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 728, in fit\n",
      "    use_multiprocessing=use_multiprocessing)\n",
      "  File \"/home/charles/anaconda3/envs/tf-2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 324, in fit\n",
      "    total_epochs=epochs)\n",
      "  File \"/home/charles/anaconda3/envs/tf-2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 123, in run_one_epoch\n",
      "    batch_outs = execution_function(iterator)\n",
      "  File \"/home/charles/anaconda3/envs/tf-2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 86, in execution_function\n",
      "    distributed_function(input_fn))\n",
      "  File \"/home/charles/anaconda3/envs/tf-2.0/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/charles/anaconda3/envs/tf-2.0/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 487, in _call\n",
      "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
      "  File \"/home/charles/anaconda3/envs/tf-2.0/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1823, in __call__\n",
      "    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\n",
      "  File \"/home/charles/anaconda3/envs/tf-2.0/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1141, in _filtered_call\n",
      "    self.captured_inputs)\n",
      "  File \"/home/charles/anaconda3/envs/tf-2.0/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1224, in _call_flat\n",
      "    ctx, args, cancellation_manager=cancellation_manager)\n",
      "  File \"/home/charles/anaconda3/envs/tf-2.0/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 511, in call\n",
      "    ctx=ctx)\n",
      "  File \"/home/charles/anaconda3/envs/tf-2.0/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\", line 61, in quick_execute\n",
      "    num_outputs)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/charles/.local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/charles/.local/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/charles/.local/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/charles/.local/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/charles/anaconda3/envs/tf-2.0/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/charles/anaconda3/envs/tf-2.0/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/charles/anaconda3/envs/tf-2.0/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/charles/anaconda3/envs/tf-2.0/lib/python3.7/inspect.py\", line 732, in getmodule\n",
      "    for modname, module in list(sys.modules.items()):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(X_train,\n",
    "                                                              padding='post',\n",
    "                                                              value=padding_value,\n",
    "                                                              dtype='float32')\n",
    "print(padded_inputs.shape)\n",
    "regressor = Sequential()\n",
    "regressor.add(Masking(mask_value=padding_value))\n",
    "regressor.add(T1TimeLSTM(units=lstm_units))\n",
    "regressor.add(Dense(units=y_train.shape[1], activation='sigmoid'))\n",
    "\n",
    "regressor.compile(optimizer='adam', loss='mean_squared_error')\n",
    "history = regressor.fit(padded_inputs, y_train, batch_size=1, epochs=number_of_epochs, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.preprocessing.sequence.pad_sequences(X_test,\n",
    "                                                              padding='post',\n",
    "                                                              value=padding_value,\n",
    "                                                              dtype='float32')\n",
    "pred = regressor.predict(padded_inputs)\n",
    "mean_sum_of_squares = sum([(y_test[i] - pred[i])**2 for i in range(dataset.training_set_length)])/dataset.training_set_length\n",
    "print(mean_sum_of_squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T2TimeLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(X_train,\n",
    "                                                                      padding='post',\n",
    "                                                                      value=padding_value,\n",
    "                                                                      dtype='float32')\n",
    "print(padded_inputs.shape)\n",
    "regressor = Sequential()\n",
    "regressor.add(Masking(mask_value=padding_value))\n",
    "regressor.add(T2TimeLSTM(units=lstm_units))\n",
    "regressor.add(Dense(units=y_train.shape[1], activation='sigmoid'))\n",
    "\n",
    "regressor.compile(optimizer='adam', loss='mean_squared_error')\n",
    "history = regressor.fit(padded_inputs, y_train, batch_size=1, epochs=number_of_epochs, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.preprocessing.sequence.pad_sequences(X_test,\n",
    "                                                              padding='post',\n",
    "                                                              value=padding_value,\n",
    "                                                              dtype='float32')\n",
    "pred = regressor.predict(padded_inputs)\n",
    "mean_sum_of_squares = sum([(y_test[i] - pred[i])**2 for i in range(dataset.training_set_length)])/dataset.training_set_length\n",
    "print(mean_sum_of_squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TimeDepJointEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_units = dataset.number_of_event\n",
    "padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(X_train,\n",
    "                                                              padding='post',\n",
    "                                                              value=padding_value,\n",
    "                                                              dtype='float32')\n",
    "\n",
    "print(padded_inputs.shape)\n",
    "regressor = Sequential()\n",
    "regressor.add(Masking(mask_value=padding_value))\n",
    "regressor.add(TimeDepJointEmbedding(units=lstm_units))\n",
    "regressor.add(Dense(units=y_train.shape[1], activation='sigmoid'))\n",
    "\n",
    "regressor.compile(optimizer='adam', loss='mean_squared_error')\n",
    "history = regressor.fit(padded_inputs, y_train, batch_size=1, epochs=number_of_epochs, verbose=2)\n",
    "X_test = np.array([padded_inputs[0, :, :]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.preprocessing.sequence.pad_sequences(X_test,\n",
    "                                                              padding='post',\n",
    "                                                              value=padding_value,\n",
    "                                                              dtype='float32')\n",
    "pred = regressor.predict(padded_inputs)\n",
    "mean_sum_of_squares = sum([(y_test[i] - pred[i])**2 for i in range(dataset.training_set_length)])/dataset.training_set_length\n",
    "print(mean_sum_of_squares)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-2.0",
   "language": "python",
   "name": "tf-2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
